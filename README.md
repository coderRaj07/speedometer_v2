# ğŸš— Real-Time Speedometer System (Production-Grade)

A **distributed, event-driven telemetry system** that ingests sensor data, stores time-series history, processes real-time state, and streams live updates to a web UI.

This project demonstrates **advanced backend system design** using Kafka, Redis, FastAPI, PostgreSQL, WebSockets, and Docker.

---

## ğŸ“Œ High-Level Architecture

```
 Sensors / Simulator
        |
        v
 Ingestion API (FastAPI)
        |
        v
     Kafka Topic
   (speed-events)
        |
        +-----------------------------+
        |                             |
        v                             v
 DB Writer Consumer          Stream Processor
 (PostgreSQL / Timeseries)   (Latest state)
        |                             |
        v                             v
  Historical Data              Redis Pub/Sub
                                      |
                                      v
                             WebSocket Gateway
                                      |
                                      v
                                 Frontend UI
```

---

## ğŸ§  Design Philosophy

* **Kafka is the backbone** (durable, replayable, scalable)
* **Redis is for real-time fan-out**, not storage
* **PostgreSQL stores history**, not live state
* **WebSocket is isolated** from ingestion load
* **Frontend is stateless and disposable**

Each service has **one responsibility**.

---

## ğŸ“‚ Folder Structure & Responsibilities

```
speedometer/
â”œâ”€â”€ backend/                 # Ingestion API
â”‚   â”œâ”€â”€ main.py               # POST /speed â†’ Kafka
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ db-writer/               # Kafka â†’ PostgreSQL
â”‚   â”œâ”€â”€ consumer.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ stream-processor/        # Kafka â†’ Redis (latest)
â”‚   â”œâ”€â”€ consumer.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ websocket-gateway/       # Redis â†’ WebSocket
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ simulator/               # Sensor simulator
â”‚   â”œâ”€â”€ sensor_simulator.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/                # React UI
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ infra/
â”‚   â””â”€â”€ postgres/
â”‚       â””â”€â”€ init.sql         # DB schema
â”‚
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---

## ğŸ”„ End-to-End Flow (Step by Step)

### 1ï¸âƒ£ Sensor / Simulator

* Periodically sends speed data

```json
{
  "sensor_id": "sensor-1",
  "speed": 42,
  "ts": 1700000000
}
```

---

### 2ï¸âƒ£ Backend â€“ Ingestion API (FastAPI)

* Accepts HTTP requests
* Validates input
* Publishes event to Kafka

```python
producer.send("speed-events", event)
```

ğŸ“Œ Backend does **not** talk to Redis or WebSockets.

---

### 3ï¸âƒ£ Kafka â€“ Event Backbone

* Topic: `speed-events`
* Guarantees:

  * Durability
  * Ordering
  * Replay
  * Backpressure

Kafka decouples producers and consumers.

---

### 4ï¸âƒ£ DB Writer â€“ Historical Storage

* Kafka consumer
* Writes events to PostgreSQL

```sql
INSERT INTO speed_readings(sensor_id, speed, ts)
```

ğŸ“Œ Purpose: **time-series history & analytics**

---

### 5ï¸âƒ£ Stream Processor â€“ Real-Time State

* Kafka consumer
* Extracts latest speed
* Publishes to Redis

```python
redis.publish("speed-updates", speed)
```

ğŸ“Œ Purpose: **latest value only**

---

### 6ï¸âƒ£ Redis â€“ Real-Time Fan-out

* Pub/Sub channel: `speed-updates`
* Ultra-fast, in-memory
* No persistence (Kafka is source of truth)

---

### 7ï¸âƒ£ WebSocket Gateway

* Subscribes to Redis
* Pushes updates to connected clients

```python
await ws.send_text(speed)
```

ğŸ“Œ Isolated from backend & Kafka load

---

### 8ï¸âƒ£ Frontend (React)

* Connects via WebSocket

```js
ws://localhost:9000/ws/speed
```

* Updates UI in real time
* No business logic

---

## ğŸ Major Bugs We Hit (and Fixed)

### âŒ 1. Kafka `NoBrokersAvailable`

**Cause**

* Containers tried `localhost:9092`
* Kafka not ready at startup

**Fix**

* Use `kafka:9092`
* Add retry loops around KafkaProducer / KafkaConsumer

---

### âŒ 2. WebSocket 404 / Upgrade Failed

**Cause**

* `uvicorn` installed without WebSocket support

**Fix**

```txt
uvicorn[standard]
```

---

### âŒ 3. Frontend Showing `0 km/h`

**Cause**

* WebSocket payload not numeric
* React silently ignored invalid `Number()`

**Fix**

* Normalize payload before sending

---

### âŒ 4. WebSocket Connected to Wrong Service

**Cause**

* Frontend still pointed to backend (`8000`)
* WebSocket moved to gateway (`9000`)

**Fix**

```js
ws://localhost:9000/ws/speed
```

---

### âŒ 5. Multiple Kafka Consumers Created

**Cause**

* Duplicate `KafkaConsumer()` calls
* Missing `group_id`

**Fix**

* Single consumer
* Proper `group_id`
* Retry logic

---

## ğŸ§ª How to Run on Linux (Clean Instructions)

### 1ï¸âƒ£ Prerequisites

```bash
sudo apt update
sudo apt install -y docker.io docker-compose
sudo systemctl enable docker
sudo systemctl start docker
```

(Optional)

```bash
sudo usermod -aG docker $USER
logout
```

---

### 2ï¸âƒ£ Clean Start (Recommended)

```bash
docker compose down -v --remove-orphans
docker system prune -a -f --volumes
```

---

### 3ï¸âƒ£ Build & Start Everything

```bash
docker compose build --no-cache
docker compose up
```

---

### 4ï¸âƒ£ Verify Services

```bash
docker compose ps
```

Expected:

* backend â†’ 8000
* websocket-gateway â†’ 9000
* frontend â†’ 3000
* kafka â†’ 9092
* redis â†’ 6379
* postgres â†’ 5432

---

### 5ï¸âƒ£ Open UI

```
http://localhost:3000
```

You should see **live speed updates**.

---

### 6ï¸âƒ£ Debugging Commands

```bash
docker compose logs -f backend
docker compose logs -f stream-processor
docker compose logs -f websocket-gateway
```

Manual Redis test:

```bash
docker exec -it speedometer-redis redis-cli
PUBLISH speed-updates 88
```

---

## ğŸ“ Why This Is Production-Grade

* Event-driven, not request-driven
* Horizontal scalability
* Clear separation of concerns
* Safe frontend deployments
* Replayable data
* Real-time + historical paths separated

This is **exactly how telemetry, IoT, fintech streams, and monitoring systems are built**.

---

## ğŸ§  One-Line Summary 

> â€œWe built a real-time telemetry system using Kafka for durability, Redis for live state, PostgreSQL for time-series storage, and a dedicated WebSocket gateway for UI updates â€” fully decoupled and production-safe.â€

---